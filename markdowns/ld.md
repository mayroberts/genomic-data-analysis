Linkage disequilibrium
================

  - [Introduction](#introduction)
  - [Data preparation](#data-preparation)
      - [Beagle formatted genotype likelihood
        file](#beagle-formatted-genotype-likelihood-file)
      - [Position file](#position-file)
  - [LD estimation](#ld-estimation)
      - [Standalone server](#standalone-server)
      - [Computer cluster](#computer-cluster)
      - [Output](#output)
      - [Notes](#notes)
  - [LD pruning](#ld-pruning)
      - [Standalone server](#standalone-server-1)
      - [Computer cluster](#computer-cluster-1)
      - [Output](#output-1)
      - [Notes](#notes-1)
  - [Example workflow](#example-workflow)

<br>

## Introduction

Linkage disequilibrium (LD) is the nonrandom association of alleles of
different loci. It has important applications, e.g. for the inference of
population size, demographic history, selection, and for the discovery
of structural variants. In addition, since many downstream analyses make
assumptions about the independence of genomic loci, LD estimates are
essential for trimming the list of loci to be included in these analyses
(LD pruning).

Here, we use the program [ngsLD](https://github.com/fgvieira/ngsLD) to
estimate LD between pairs of loci based on genotype likelihoods. We then
use a perl sript provided in ngsLD to perform LD pruning.

<br>

## Data preparation

You will need to prepare two files for LD estimation. Both are outputted
from ANGSD (e.g. the SNP calling step, or the minor allele frequency
step if you want to estimate LD per population), but they may require
some modifications.

<br>

#### Beagle formatted genotype likelihood file

NgsLD takes a beagle formatted genotype likelihood file one of the input
files. However, depending on your goal in LD estimation, you may need to
modify this `beagle.gz` file first before importing it into ngsLD.
Specifically, if your goal is to estimate the rate of LD decay, it may
be ok to start with the original beagle file.

However, if the goal is to visualize LD blocks and/or to perform LD
pruning, directly taking the `beagle.gz` file outputted by ANGSD may
lead to several issues.

  - We do not recommend you to randomly subsample the data using ngsLD
    (`--rnd_sample`), because it will lead to missing data between some
    pairs of loci, which is problematic for visualization and can cause
    incomplete LD pruning. And since the original `beagle.gz` file tends
    to be very large, the computational burden can be quite high if you
    don’t subsample it first yourself.

  - We found that including the `beagle.gz` file header line and its
    first three columns may lead to problems. This may be due to a bug
    in an older version of ngsLD, or a error in our part, but we know
    for sure that ngsLD works properly if the header line and the first
    three columns are removed.

Therefore, for LD block visualization and LD pruning, we recommend you
to perform the following preprocessing on the ANGSD-outputted
`beagle.gz` file.

  - Subsample the file so that it only contains a manageable number of
    SNPs (e.g. we found 2 million to be a good number, but this also
    depends on your sample size and the level of LD in your system).

  - Remove the first line and the first three columns.

These can be done with command lines like the following.

``` bash
## Select one line in every 5 lines (header will be removed this way), and remove the first three columns
zcat original.beagle.gz | awk 'NR % 5 == 0' | cut -f 4- | gzip  > subsampled.beagle.gz
```

<br>

#### Position file

You may have noticed that the identity of the SNPs are missing from the
`beagle.gz` file after the first three columns are removed. Indeed, you
will need to input the SNP positions as a separate file. We recommend
you to use the first two columns of the `mafs.gz` file generated by
ANGSD as the position file. If your beagle file is subsampled, please
make sure that the same SNPs are subsampled from the `mafs.gz` file. For
example,

``` bash
## Select the first two columns, and one line in every 5 lines (header will be removed this way)
zcat original.mafs.gz | cut -f 1,2 |  awk 'NR % 50 == 0' | gzip > subsampled.txt.gz
```

The resulting position file should have exactly the same number of lines
as the `beagle.gz` file.

<br>

## LD estimation

Once you have the beagle formatted genotype likelihood file and the
position file in the right configuration, we can proceed to LD
estimation.

<br>

#### Standalone server

Run the
[get\_ld.sh](https://github.com/therkildsen-lab/genomic-data-analysis/blob/master/scripts/get_ld.sh)
script with `nohup bash` and pass the following input variables as
positional parameters **in the given order**:

1.  `INPUT_PATH`: Path to directory containing the input files, i.e. the
    beagle formatted genotype likelihood file and SNP position file
    (e.g. `/workdir/cod/greenland-cod/angsd/`)
2.  `GENO`: Name of the input genotype likelihood file
    (e.g. `bam_list_realigned_mincov_contamination_filtered_mindp368_maxdp928_minind167_minq20_downsampled.beagle.gz`)
3.  `POS`: Tab deliminated list of SNPs with chromosome name in the
    first column and position in the second column
    (e.g. `global_snp_list_bam_list_realigned_mincov_contamination_filtered_mindp368_maxdp928_minind167_minq20_downsampled.txt.gz`)
4.  `MAXDIST`: Maximum distance for analyses in kB (e.g. `15`)
5.  `THREADS`: Number of parallel threads to use (default value is `8`,
    but the program can use a lot more if they are made available)
6.  `NGSLD`: Path to ngsLD (default value is
    `/programs/ngsLD-1.1.1/ngsLD`)
7.  `EXTRA_ARG`: Extra arguments provided to ngsLD (default value is
    `''`)

Note that if you would like to, for example, specify the path to ngsLD
yourself, you will need to specify all variables before it, even if some
of them have a default value (e.g. `THREADS`)

Below is an example taken from the Greenland cod project:

``` bash
nohup bash /workdir/genomic-data-analysis/scripts/get_ld.sh \
/workdir/cod/greenland-cod/angsd/ \
bam_list_realigned_mincov_contamination_filtered_mindp368_maxdp928_minind167_minq20_downsampled.beagle.gz \
global_snp_list_bam_list_realigned_mincov_contamination_filtered_mindp368_maxdp928_minind167_minq20_downsampled.txt.gz \
15 \
8 \
/programs/ngsLD-1.1.1/ngsLD \
> /workdir/cod/greenland-cod/nohups/run_ngsLD_bam_list_realigned_mincov_contamination_filtered_mindp368_maxdp928_minind167_minq20_downsampled.nohup &
```

<br>

#### Computer cluster

The LD estimation step in ngsLD is highly parallelizable, so we will not
split the process into smaller tasks ourselves, and will rely on the
`--n_threads` option in ngsLD to take advantage of multiple CPU cores in
a computer cluster.

Therefore, to run LD estimation on a cluster, we will first move the
input files to the computing node, and will run the
[get\_ld.sh](https://github.com/therkildsen-lab/genomic-data-analysis/blob/master/scripts/get_ld.sh)
script as we do in the standalone server case. The advantage is that we
may be able to request a larger number of CPU cores and speed us
computation considerarbly.

Below is an example taken from the Greenland cod project. To make this
script fit your project and computing system, you will need to copy this
script and make several modifications in lines above the line of hash
marks (`#####`). The content below the line of hashmarks does not need
to be altered.

  - You will need to modify some of the slurm headers to fit your
    scheduler and cluster system. E.g.
    
      - location of the log file (`--output`)
      - the queue to submit your job to `--partition`
      - the number of CPU cores (`--ntasks`) and memory (`--mem`) to
        request

  - You will need to mount the appropriate server where your input data
    is stored.

  - You will need to define the same variables as in case of a
    standalone server. Make sure that the path should be relative to the
    computing node, not the node that you submit the job from.
    
      - There is an extra variable, `SCRIPT`, which is the location
        where the `get_ld.sh` script is stored (relative to the
        computing node)
      - It is important to make sure the you request the same number of
        CPU cores (`--ntasks` in slurm) as you specify ngsLD to use
        (`THREADS`)

<!-- end list -->

``` bash
echo '#!/bin/bash
#SBATCH --job-name=ld_estimation
#SBATCH --output=/home/rl683/slurm/log/ld_estimation.log
#SBATCH --partition=long7
#SBATCH --nodes=1
#SBATCH --ntasks=50
#SBATCH --mem=50G

## Mount nt246
/programs/bin/labutils/mount_server cbsunt246 /workdir

## Define some variables
INPUT_PATH=/fs/cbsunt246/workdir/cod/greenland-cod/angsd/
GENO=bam_list_realigned_mincov_contamination_filtered_mindp368_maxdp928_minind167_minq20_downsampled.beagle.gz
POS=global_snp_list_bam_list_realigned_mincov_contamination_filtered_mindp368_maxdp928_minind167_minq20_downsampled.txt.gz
NGSLD=/fs/cbsunt246/workdir/programs/ngsLD-1.1.0/ngsLD
MAXDIST=15 
THREADS=50
EXTRA_ARG=''
SCRIPT=/fs/cbsunt246/workdir/genomic-data-analysis/scripts/get_ld.sh

##################################################

## Keep a record of the Job ID
echo $SLURM_JOB_ID

## Create and move to working directory for job
WORKDIR=/workdir/$USER/$SLURM_JOB_ID/
mkdir -p $WORKDIR
cd $WORKDIR

## Transfer the input files
cp $INPUT_PATH$GENO $WORKDIR
cp $INPUT_PATH$POS $WORKDIR

## Define the output name
OUT=${GENO%%.*}.ld

## Run get_ld.sh
bash $SCRIPT \
$WORKDIR \
$GENO \
$POS \
$MAXDIST \
$THREADS \
$NGSLD \
$EXTRA_ARG

## Move output files back
cp $WORKDIR$OUT $INPUT_PATH' | sbatch
```

<br>

#### Output

The output of LD estimation is a tab-deliminated file with a `.ld`
suffix. The first two columns are the positions of the pair of SNPs, the
third column is the distance between them, and the 7th column is LD
measured in r-squared. Please see the ngsLD website for details.

<br>

#### Notes

  - Some useful extra arguments that can be passed to ngsLD during the
    LD estimation step include:
    
      - `--min_maf`: minimum SNP minor allele frequency
      - `--rnd_sample`: proportion of comparisons to randomly sample;
        this can be particularly useful if the goal is just to estimate
        the rate of LD decay

  - The `MAXDIST` variable (or `--max_kb_dist` in ngsLD) is an important
    one to set. Here are a few considerations when setting this
    variable.
    
      - This should be the distance beyond which LD levels off and two
        sites can be assumed to be unlinked.
      - When set too high, the computational burden can become too high.
        But when set too low, you may miss an important part of the LD
        decay curve, and/or the LD pruning may be incomplete.
      - One strategy, therefore, is to first set up a high `MAXDIST`
        value for LD estimation while using the `--rnd_sample` option to
        drastically subsample. This is will quickly give you a sense of
        the rate of LD decay in the data. Then, you will be able to set
        up a more reasonable `MAXDIST` threshold.
      - On the other hand, if you have a rough sense of the rate of LD
        decay in your system, you can just set up the `MAXDIST` value
        based on your guess (maybe erring towards the larger values). If
        you then find that LD is still high at this distance, you can
        rerun LD estimation with a higher `MAXDIST` value.

## LD pruning

LD pruning is the removal of SNPs from the dataset such that no pairs of
SNPs exhibit high LD. We will use the `prune_graph.pl` script to perform
LD pruning. It takes the `.ld` file that ngsLD generated as input.

<br>

#### Standalone server

First, define the following variables in your Unix environment.

  - `INPUT_PATH`: Path to directory containing the input files,
    i.e. `.ld` file generated from the LD estimation step
    (e.g. `/workdir/cod/greenland-cod/angsd/`)
  - `LD`: Name of the input file
    (e.g. `bam_list_realigned_mincov_contamination_filtered_mindp368_maxdp928_minind167_minq20_downsampled.ld`)
  - `MAXDIST`: Maximum distance for pruning in kB; SNPs beyond this
    distance are considered unlinked (e.g. `15`)
  - `MINWEIGHT`: Minimum r-squared value for two SNPs to be considered
    linked; when considering to be linked, the pair of SNPs become the
    target of LD pruning (e.g. `0.5`)
  - `PRUNE_GRAPH`: Path to `prune_graph.pl`
    (e.g. `/programs/ngsLD-1.1.1/scripts/prune_graph.pl`)

For example, you will run the following lines to define these variables.

``` bash
INPUT_PATH=/workdir/cod/greenland-cod/angsd/
LD=bam_list_realigned_mincov_contamination_filtered_mindp368_maxdp928_minind167_minq20_downsampled.ld
MAXDIST=15 
MINWEIGHT=0.5
PRUNE_GRAPH=/programs/ngsLD-1.1.1/scripts/prune_graph.pl
```

Then, you will run the following script to perform LD pruning.

``` bash
nohup perl $PRUNE_GRAPH \
--in_file $INPUT_PATH$LD \
--max_kb_dist $MAXDIST \
--min_weight $MINWEIGHT \
--out $INPUT_PATH${LD%%.*}_unlinked.id &
```

Unlike the LD estimation step, `prune_graph.pl` does not multi-thread at
all. Therefore, LD pruning may take a very long time to run. One option,
therefore, is to split the run by chromosomes. Next, we implement this
multi-threading method for computer clusters.

<br>

#### Computer cluster

Here, we take advantage the array jobs functionality on a slurm cluster,
and split chromosomes into arrays. Again, you will need to copy the
script below and make several modifications in lines above the line of
hash marks (`#####`). The content below the line of hashmarks does not
need to be altered.

  - You will need to modify some of the slurm headers to fit your
    scheduler and cluster system. E.g.
    
      - location of the log file (`--output`)
      - the queue to submit your job to `--partition`
      - the number of array jobs to submit (`--array`). This should be 1
        to the number of chromosomes that you want to perform LD pruning
        on (e.g. `1-23`)
      - the number of CPU cores (`--ntasks`) and memory (`--mem`) to
        request for each array; `--ntasks` should always be `1` in this
        case, because the `prune_graph.pl` script cannot use multiple
        threads

  - You will need to mount the appropriate server where your input data
    is stored.

  - You will need to define the same variables as in case of a
    standalone server. Make sure that the path should be relative to the
    computing node, not the node that you submit the job from.
    
      - There is an extra variable, `LG_LIST`, which is a list of
        chromosomes that you want to perform LD pruning on. Each line
        should be a name of a chromosome
        (e.g. `/fs/cbsunt246/workdir/cod/greenland-cod/sample_lists/lg_list.txt`)
      - Again, it is important to make sure the you specify as many
        arrays (`--array`) as the number of lines in `LG_LIST`

<!-- end list -->

``` bash
echo '#!/bin/bash
#SBATCH --job-name=ld_pruning
#SBATCH --output=/home/rl683/slurm/log/ld_pruning_greenland_by_lg.log
#SBATCH --partition=long7
#SBATCH --nodes=1
#SBATCH --array=1-23
#SBATCH --ntasks=1
#SBATCH --mem=20G

## Mount nt246
/programs/bin/labutils/mount_server cbsunt246 /workdir

## Define some variables
INPUT_PATH=/fs/cbsunt246/workdir/cod/greenland-cod/angsd/
LD=bam_list_realigned_mincov_contamination_filtered_mindp368_maxdp928_minind167_minq20_downsampled.ld
MAXDIST=15 
MINWEIGHT=0.5
PRUNE_GRAPH=/programs/ngsLD-1.1.1/scripts/prune_graph.pl
LG_LIST=/fs/cbsunt246/workdir/cod/greenland-cod/sample_lists/lg_list.txt

##################################################

## Keep a record of the Job ID
echo $SLURM_JOB_ID
## Create and move to working directory for job
WORKDIR=/workdir/$USER/$SLURM_JOB_ID-$SLURM_ARRAY_TASK_ID/
mkdir -p $WORKDIR
cd $WORKDIR
## Transfer the input files
cp $INPUT_PATH$LD $WORKDIR
## Select the LG from the input file
LG=`head $LG_LIST -n $SLURM_ARRAY_TASK_ID | tail -n 1`
grep ^${LG}: $WORKDIR$LD > $WORKDIR${LD%%.*}_${LG}.ld
## Define the output name
OUT=${LD%%.*}_unlinked_${LG}.id
## Run the perl script
perl $PRUNE_GRAPH \
--in_file $WORKDIR${LD%%.*}_${LG}.ld \
--max_kb_dist $MAXDIST \
--min_weight $MINWEIGHT \
--out $WORKDIR$OUT
## Move output files back
cp $WORKDIR$OUT $INPUT_PATH
rm -r $WORKDIR' | sbatch
```

<br>

#### Output

The output is one (in the case of the standalone server) or several (in
the case of computer cluster) `.id` files, which contain the postions of
unlinked SNPs. You can use these files to further subset your SNP list,
genotype likelihood files, and other files, for different downsteam
analyses that take unlinked SNPs as input.

<br>

#### Notes

  - You may need to install some modules in perl in order to run the
    `prune_graph.pl` script. Just run the script as instructed above,
    and if you see the job ends immediately, follow the directions in
    the log file to install these perl modules. Sometimes, you may
    encounter some permission issues when installing these, and if this
    is the case, contact the people managing your computing system for
    help.

  - As noted in the “Output” section, you will get a list of unlinked
    SNPs with after LD pruning, but you will still need to select these
    SNPs in the global SNP list and `beagle.gz` file for downstream
    analysis.
    
      - Subsetting the SNP list is easy; you can just read the global
        SNP list and the list of unlinked SNPs in R, and use something
        like `semi_join()` to filter out linked SNPs from the global SNP
        list.
      - However, `beagle.gz` files tend to be very large and cannot be
        easily manipulated with R. We do not have a perfect solution to
        this either, but one idea is to output the line numbers that you
        would like to keep as a text file, and use `awk` to select only
        these lines in the `beagle.gz` file. See the next section for an
        example of how we did this.

<br>

## Example workflow

  - [LD estimation and pruning with the Greenland cod
    project](https://github.com/therkildsen-lab/greenland-cod/blob/master/markdowns/ld.md)

<br>
